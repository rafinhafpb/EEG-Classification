{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data gathering and separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "../dataverse_files\\h01.edf\n"
     ]
    }
   ],
   "source": [
    "# Get the data set path file\n",
    "all_file_path = glob('../dataverse_files/*.edf')\n",
    "print(len(all_file_path))\n",
    "print(all_file_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../dataverse_files\\h01.edf\n",
      "../dataverse_files\\s01.edf\n"
     ]
    }
   ],
   "source": [
    "# Separate healthy and schizofrenic patients eeg data\n",
    "healthy_file_path = [i for i in all_file_path if 'h' in i.split('\\\\')[1]]\n",
    "patient_file_path = [i for i in all_file_path if 's' in i.split('\\\\')[1]]\n",
    "\n",
    "print(healthy_file_path[0])\n",
    "print(patient_file_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the data from .edf files using mne\n",
    "\n",
    "def read_data(file_path, low_freq=0.5, high_freq=45, duration=5, overlap=1):\n",
    "    data = mne.io.read_raw_edf(file_path, preload=True)\n",
    "    data.set_eeg_reference()\n",
    "    data.filter(l_freq=low_freq, h_freq=high_freq)\n",
    "    epochs = mne.make_fixed_length_epochs(data, duration=duration, overlap=overlap)\n",
    "    array = epochs.get_data()\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from c:\\Projects\\EEG Classification\\dataverse_files\\h01.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 231249  =      0.000 ...   924.996 secs...\n",
      "EEG channel type selected for re-referencing\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-6 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 1651 samples (6.604 s)\n",
      "\n",
      "Not setting metadata\n",
      "231 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 231 events and 1250 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(231, 19, 1250)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = read_data(healthy_file_path[0])\n",
    "sample_data.shape   # Nb of epochs (time windows speficied by duration in the function above), channels (nb of electrodes in the eeg), lenght of the signal (nb of total points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "control_epochs_array = [read_data(i) for i in healthy_file_path]\n",
    "patient_epochs_array = [read_data(i) for i in patient_file_path]\n",
    "\n",
    "# each array will have the data of 14 subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labeling healthy data as \"0\" and schizofrenic patient data as \"1\" for each epoch on each subject\n",
    "\n",
    "control_epochs_labels = [len(i)*[0] for i in control_epochs_array]\n",
    "patient_epochs_labels = [len(i)*[1] for i in patient_epochs_array]\n",
    "\n",
    "len(control_epochs_labels), len(patient_epochs_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = control_epochs_array + patient_epochs_array    # all data\n",
    "label_list = control_epochs_labels + patient_epochs_labels  # all labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# Identifying data per subject\n",
    "group_list = [[i]*len(j) for i,j in enumerate(data_list)]\n",
    "print(len(group_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7201, 19, 1250) (7201,) (7201,)\n"
     ]
    }
   ],
   "source": [
    "# Transforms into array of eeg signals stacked\n",
    "\n",
    "data_array = np.vstack(data_list)\n",
    "label_array = np.hstack(label_list)\n",
    "group_array = np.hstack(group_list)\n",
    "print(data_array.shape, label_array.shape, group_array.shape)   # (epochs, electrodes, total points), (labels - 0 for healthy, 1 for patient), (subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_time_domain(signal):\n",
    "    \"\"\"\n",
    "    This function extracts features in the time domain from a raw EEG signal\n",
    "\n",
    "    ## Returns:\n",
    "\n",
    "    This function returns a **ndarray** which contain, respectively, the following features:\\n\n",
    "\n",
    "    - **Mean**: The average of the signal.\n",
    "\n",
    "    - **Variance**: Indicates how much the signal amplitude fluctuates from the mean.\n",
    "\n",
    "    - **Standard Deviation**: The square root of the variance, showing the average amount of deviation from the mean.\n",
    "\n",
    "    - **Root-Mean-Square (RMS)**: Measures the magnitude of the EEG signal by taking the square root of the average of the squared values.\n",
    "\n",
    "    - **Absolute Mean Difference (AMD)**: This feature calculates the average absolute difference between consecutive values.\n",
    "\n",
    "    - **Skewness**: Measures the asymmetry of the signal around the mean. Positive skew indicates a tail on the right side,\n",
    "    while negative skew shows a tail on the left. It helps detect irregular patterns.\n",
    "\n",
    "    - **Kurtosis**: Measures the \"tailedness\" of the signal distribution, indicating whether data points are close to the mean or more spread out.\n",
    "    High kurtosis may indicate spikes or irregularities in the signal.\n",
    "\n",
    "    ### Hjorth Parameters: A set of metrics specifically for EEG data:\\n\n",
    "\n",
    "    - **Mobility**: Indicates the frequency, defined as the square root of the variance of the first derivative divided by the variance of the signal.\n",
    "\n",
    "    - **Complexity**: The ratio of the mobility of the first derivative to the mobility of the signal itself.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    mean = np.mean(signal, axis=-1)\n",
    "    variance = np.var(signal, axis=-1)\n",
    "    std_dev = np.std(signal, axis=-1)\n",
    "    rms_value = np.sqrt(np.mean(signal**2, axis=-1))\n",
    "    amd_value = np.sum(np.abs(np.diff(signal, axis=-1)), axis=-1)\n",
    "\n",
    "    from scipy.stats import skew, kurtosis\n",
    "    skewness = skew(signal, axis=-1)\n",
    "    kurtosis_value = kurtosis(signal, axis=-1)\n",
    "\n",
    "    first_derivative = np.diff(signal, axis=-1)\n",
    "    second_derivative = np.diff(first_derivative, axis=-1)\n",
    "    mobility = np.sqrt(np.var(first_derivative)/variance)\n",
    "    complexity = np.sqrt(np.var(second_derivative) / np.var(first_derivative)) / mobility\n",
    "\n",
    "    return np.concatenate((mean, variance, std_dev, rms_value, amd_value, skewness, kurtosis_value, mobility, complexity), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = []\n",
    "for data in data_array:\n",
    "    all_features.append(extract_features_time_domain(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7201, 171)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features_array = np.array(all_features)\n",
    "all_features_array.shape          # (Nb of epochs, all features extracted from each electrode: 9x19) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**: Logistic regression is a supervised learning algorithm used for binary or multiclass classification. This code creates a machine learning pipeline to train a logistic regression classifier with hyperparameter tuning using grouped cross-validation.\n",
    "\n",
    "*GroupKFold Cross-Validation*:\n",
    "- GroupKFold ensures that samples from the same group (e.g., signals from the same subject) are not mixed between training and test sets. This prevents data leakage and ensures that the model generalizes better to unseen data, improving its predictive performance.\n",
    "- This approach helps when the data has inherent group structure (such as multiple signals from the same individual or session).\n",
    "\n",
    "*GridSearchCV with Hyperparameter Tuning*:\n",
    "- GridSearchCV systematically evaluates different values of the hyperparameters (in this case, the regularization strength **C** in logistic regression) and identifies the best configuration for the model.\n",
    "- The C parameter controls the tradeoff between fitting the model too closely to the training data (overfitting) and underfitting the data.\n",
    "\n",
    "*Standardization*:\n",
    "- The pipeline includes a StandardScaler, which standardizes the features (i.e., transforms them to have zero mean and unit variance). This is particularly helpful for models like logistic regression, which can be sensitive to the scale of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=GroupKFold(n_splits=5),\n",
       "             estimator=Pipeline(steps=[(&#x27;scalar&#x27;, StandardScaler()),\n",
       "                                       (&#x27;clf&#x27;, LogisticRegression())]),\n",
       "             n_jobs=12,\n",
       "             param_grid={&#x27;clf__C&#x27;: array([0.05      , 0.08275862, 0.11551724, 0.14827586, 0.18103448,\n",
       "       0.2137931 , 0.24655172, 0.27931034, 0.31206897, 0.34482759,\n",
       "       0.37758621, 0.41034483, 0.44310345, 0.47586207, 0.50862069,\n",
       "       0.54137931, 0.57413793, 0.60689655, 0.63965517, 0.67241379,\n",
       "       0.70517241, 0.73793103, 0.77068966, 0.80344828, 0.8362069 ,\n",
       "       0.86896552, 0.90172414, 0.93448276, 0.96724138, 1.        ])})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-33\" type=\"checkbox\" ><label for=\"sk-estimator-id-33\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=GroupKFold(n_splits=5),\n",
       "             estimator=Pipeline(steps=[(&#x27;scalar&#x27;, StandardScaler()),\n",
       "                                       (&#x27;clf&#x27;, LogisticRegression())]),\n",
       "             n_jobs=12,\n",
       "             param_grid={&#x27;clf__C&#x27;: array([0.05      , 0.08275862, 0.11551724, 0.14827586, 0.18103448,\n",
       "       0.2137931 , 0.24655172, 0.27931034, 0.31206897, 0.34482759,\n",
       "       0.37758621, 0.41034483, 0.44310345, 0.47586207, 0.50862069,\n",
       "       0.54137931, 0.57413793, 0.60689655, 0.63965517, 0.67241379,\n",
       "       0.70517241, 0.73793103, 0.77068966, 0.80344828, 0.8362069 ,\n",
       "       0.86896552, 0.90172414, 0.93448276, 0.96724138, 1.        ])})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-34\" type=\"checkbox\" ><label for=\"sk-estimator-id-34\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scalar&#x27;, StandardScaler()), (&#x27;clf&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-35\" type=\"checkbox\" ><label for=\"sk-estimator-id-35\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-36\" type=\"checkbox\" ><label for=\"sk-estimator-id-36\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=GroupKFold(n_splits=5),\n",
       "             estimator=Pipeline(steps=[('scalar', StandardScaler()),\n",
       "                                       ('clf', LogisticRegression())]),\n",
       "             n_jobs=12,\n",
       "             param_grid={'clf__C': array([0.05      , 0.08275862, 0.11551724, 0.14827586, 0.18103448,\n",
       "       0.2137931 , 0.24655172, 0.27931034, 0.31206897, 0.34482759,\n",
       "       0.37758621, 0.41034483, 0.44310345, 0.47586207, 0.50862069,\n",
       "       0.54137931, 0.57413793, 0.60689655, 0.63965517, 0.67241379,\n",
       "       0.70517241, 0.73793103, 0.77068966, 0.80344828, 0.8362069 ,\n",
       "       0.86896552, 0.90172414, 0.93448276, 0.96724138, 1.        ])})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model itself\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Ensures that the samples within the same group remain in the same fold during training and testing\n",
    "gkf = GroupKFold(5)\n",
    "\n",
    "# Standardizes features and puts the classifier after the scaler so the data is standardized before training.\n",
    "pipeline = Pipeline([('scalar', StandardScaler()), ('clf', clf)])\n",
    "\n",
    "# Specifies the values to test for the logistic regression C parameter. Each value in this list corresponds to a different regularization strength.\n",
    "param_grid = {'clf__C':np.linspace(0.05, 1, 30)}\n",
    "\n",
    "# GridSearchCV performs a grid search over the param_grid to find the best C parameter for the logistic regression model, \n",
    "# using cross-validation with GroupKFold. The n_jobs parameter allows parallel processing, speeding up the computation by using X CPU cores\n",
    "gscv = GridSearchCV(pipeline, param_grid, cv=gkf, n_jobs=12)\n",
    "\n",
    "# trains the GridSearchCV object using the feature array (all_features_array), labels (label_array), and groups (group_array). \n",
    "# After running this, gscv will contain the best logistic regression model based on the cross-validated performance.\n",
    "gscv.fit(all_features_array, label_array, groups=group_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score Prediction: 70.52 %\n",
      "Best parameter (C): {'clf__C': 0.1482758620689655}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Score Prediction: {:.2f} %\".format(gscv.best_score_*100))\n",
    "\n",
    "print(\"Best parameter (C):\", gscv.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
