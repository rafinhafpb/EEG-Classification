{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data gathering and separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "../dataverse_files\\h01.edf\n"
     ]
    }
   ],
   "source": [
    "# Get the data set path file\n",
    "all_file_path = glob('../dataverse_files/*.edf')\n",
    "print(len(all_file_path))\n",
    "print(all_file_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../dataverse_files\\h01.edf\n",
      "../dataverse_files\\s01.edf\n"
     ]
    }
   ],
   "source": [
    "# Separate healthy and schizofrenic patients eeg data\n",
    "healthy_file_path = [i for i in all_file_path if 'h' in i.split('\\\\')[1]]\n",
    "patient_file_path = [i for i in all_file_path if 's' in i.split('\\\\')[1]]\n",
    "\n",
    "print(healthy_file_path[0])\n",
    "print(patient_file_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the data from .edf files using mne\n",
    "\n",
    "def read_data(file_path, low_freq=0.5, high_freq=45, duration=5, overlap=1):\n",
    "    data = mne.io.read_raw_edf(file_path, preload=True)\n",
    "    data.set_eeg_reference()\n",
    "    data.filter(l_freq=low_freq, h_freq=high_freq)\n",
    "    epochs = mne.make_fixed_length_epochs(data, duration=duration, overlap=overlap)\n",
    "    array = epochs.get_data()\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from c:\\Projects\\EEG Classification\\dataverse_files\\h01.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 231249  =      0.000 ...   924.996 secs...\n",
      "EEG channel type selected for re-referencing\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-6 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 1651 samples (6.604 s)\n",
      "\n",
      "Not setting metadata\n",
      "231 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 231 events and 1250 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(231, 19, 1250)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = read_data(healthy_file_path[0])\n",
    "sample_data.shape   # Nb of epochs (time windows speficied by duration in the function above), channels (nb of electrodes in the eeg), lenght of the signal (nb of total points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "control_epochs_array = [read_data(i) for i in healthy_file_path]\n",
    "patient_epochs_array = [read_data(i) for i in patient_file_path]\n",
    "\n",
    "# each array will have the data of 14 subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labeling healthy data as \"0\" and schizofrenic patient data as \"1\" for each epoch on each subject\n",
    "\n",
    "control_epochs_labels = [len(i)*[0] for i in control_epochs_array]\n",
    "patient_epochs_labels = [len(i)*[1] for i in patient_epochs_array]\n",
    "\n",
    "len(control_epochs_labels), len(patient_epochs_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = control_epochs_array + patient_epochs_array    # all data\n",
    "label_list = control_epochs_labels + patient_epochs_labels  # all labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# Identifying data per subject\n",
    "group_list = [[i]*len(j) for i,j in enumerate(data_list)]\n",
    "print(len(group_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7201, 19, 1250) (7201,) (7201,)\n"
     ]
    }
   ],
   "source": [
    "# Transforms into array of eeg signals stacked\n",
    "\n",
    "data_array = np.vstack(data_list)\n",
    "label_array = np.hstack(label_list)\n",
    "group_array = np.hstack(group_list)\n",
    "print(data_array.shape, label_array.shape, group_array.shape)   # (epochs, electrodes, total points), (labels - 0 for healthy, 1 for patient), (subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Extraction: \n",
    "### Time Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def extract_features_time_domain(signal):\n",
    "    \"\"\"\n",
    "    This function extracts features in the time domain from a raw EEG signal\n",
    "\n",
    "    ## Parameters:\n",
    "    **signal**: *array_like*\\n\n",
    "    The array representing the signal to extract the features.\n",
    "\n",
    "    ## Returns:\n",
    "\n",
    "    This function returns a *ndarray* which contain, respectively, the following features:\\n\n",
    "\n",
    "    1. **Mean**: The average of the signal.\n",
    "\n",
    "    2. **Variance**: Indicates how much the signal amplitude fluctuates from the mean.\n",
    "\n",
    "    3. **Standard Deviation**: The square root of the variance, showing the average amount of deviation from the mean.\n",
    "\n",
    "    4. **Root-Mean-Square (RMS)**: Measures the magnitude of the EEG signal by taking the square root of the average of the squared values.\n",
    "\n",
    "    5. **Absolute Mean Difference (AMD)**: This feature calculates the average absolute difference between consecutive values.\n",
    "\n",
    "    6. **Skewness**: Measures the asymmetry of the signal around the mean. Positive skew indicates a tail on the right side,\n",
    "    while negative skew shows a tail on the left. It helps detect irregular patterns.\n",
    "\n",
    "    7. **Kurtosis**: Measures the \"tailedness\" of the signal distribution, indicating whether data points are close to the mean or more spread out.\n",
    "    High kurtosis may indicate spikes or irregularities in the signal.\n",
    "\n",
    "    ### Hjorth Parameters: A set of metrics specifically for EEG data:\\n\n",
    "\n",
    "    8. **Mobility**: Indicates the frequency, defined as the square root of the variance of the first derivative divided by the variance of the signal.\n",
    "\n",
    "    9. **Complexity**: The ratio of the mobility of the first derivative to the mobility of the signal itself.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    mean = np.mean(signal, axis=-1)\n",
    "    variance = np.var(signal, axis=-1)\n",
    "    std_dev = np.std(signal, axis=-1)\n",
    "    rms_value = np.sqrt(np.mean(signal**2, axis=-1))\n",
    "    amd_value = np.sum(np.abs(np.diff(signal, axis=-1)), axis=-1)\n",
    "\n",
    "    skewness = skew(signal, axis=-1)\n",
    "    kurtosis_value = kurtosis(signal, axis=-1)\n",
    "\n",
    "    first_derivative = np.diff(signal, axis=-1)\n",
    "    second_derivative = np.diff(first_derivative, axis=-1)\n",
    "    mobility = np.sqrt(np.var(first_derivative)/variance)\n",
    "    complexity = np.sqrt(np.var(second_derivative) / np.var(first_derivative)) / mobility\n",
    "\n",
    "    return np.concatenate((mean, variance, std_dev, rms_value, amd_value, skewness, kurtosis_value, mobility, complexity), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# According to database: sampling frequency = 250 Hz\n",
    "\n",
    "def compute_psd(signal, fs, window_lenght=256):\n",
    "    \"\"\"\n",
    "    This function extracts the **Power Spectral Density (PSD)** from a raw EEG signal. The PSD \n",
    "    represents the power of each frequency component in the signal and it gives insights into \n",
    "    the signal's energy distribution across different frequencies.\n",
    "\n",
    "    ## Parameters:\n",
    "        **signal**: *array_like*\\n\n",
    "        The EEG signal.\n",
    "\n",
    "        **fs**: *int*\\n\n",
    "        The sampling frequency in which the signal was acquired.\n",
    "        \n",
    "        **window_lenght**: *int, optional*\\n\n",
    "        Length of each segment. Defaults to 256.\n",
    "\n",
    "    ## Returns:\n",
    "        **freqs**: *ndarray*\\n \n",
    "        Array of sample frequencies.\n",
    "\n",
    "        **psd**: *ndarray*\\n \n",
    "        Power spectral density.\n",
    "    \"\"\"\n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=window_lenght, axis=-1)\n",
    "\n",
    "    return freqs, psd\n",
    "\n",
    "\n",
    "def bandpower(freqs, psd, band):\n",
    "    \"\"\"\n",
    "    This function extracts the **integral** (or sum) of the PSD within a specific frequency band (e.g., delta, theta, etc.).\n",
    "\n",
    "    ## Parameters:\n",
    "        **freqs**: *array_like*\\n\n",
    "        Array of frequencies.\n",
    "\n",
    "        **psd**: *array_like*\\n\n",
    "        Power spectral density of the frequencies.\n",
    "        \n",
    "        **band**: *list*\\n\n",
    "        List containing the first and last value of the band. E.g.: (0.5, 4)\n",
    "\n",
    "    ## Returns:\n",
    "        **band_power**: *int or float*\\n \n",
    "        The band power (integral of PSD) within the band.\n",
    "    \"\"\"\n",
    "    # Identify frequencies within the band\n",
    "    idexes_band = np.logical_and(freqs >= band[0], freqs <= band[1])\n",
    "    # Integrate the power spectral density (sum the area under the curve)\n",
    "    band_power = np.trapz(psd[idexes_band], freqs[idexes_band])\n",
    "\n",
    "    return band_power\n",
    "\n",
    "\n",
    "def mean_median_freq(freqs, psd):\n",
    "    \"\"\"\n",
    "    This function calculates the Mean and Median Frequency of a signal.\n",
    "\n",
    "    The Mean frequency is the average frequency weighted by power, which gives a sense of\n",
    "    where most of the signal's power is concentrated.\n",
    "\n",
    "    The Median Frequency is the frequency that divides the PSD into two halves (50% of the total power below and 50% above).\n",
    "\n",
    "    ## Parameters:\n",
    "        **freqs**: *array_like*\\n\n",
    "        Array of frequencies.\n",
    "\n",
    "        **psd**: *array_like*\\n\n",
    "        Power spectral density of the frequencies.\n",
    "\n",
    "    ## Returns:\n",
    "        **mean_median**: *ndarray*\\n\n",
    "        The array containing the Mean and Median values as first and second positions, respectively.\n",
    "    \"\"\"\n",
    "    mean_freq = np.average(freqs, weights=psd)      # Weighted average\n",
    "    cumulative_power = np.cumsum(psd)               # Cumulative sum of the power\n",
    "    median_freq = freqs[np.searchsorted(cumulative_power, round(cumulative_power[-1]/2))]  # Calculates the Median frequency\n",
    "\n",
    "    return np.array([mean_freq, median_freq])\n",
    "\n",
    "\n",
    "def spectral_entropy(psd):\n",
    "    \"\"\"\n",
    "    This function calculates the Spectral Entropy of a signal. It measures the complexity\n",
    "    or unpredictability of the signal in the frequency domain. Higher entropy suggests a more complex signal.\n",
    "\n",
    "    ## Parameters:\n",
    "        **psd**: *array_like*\\n\n",
    "        Power spectral density of the signal.\n",
    "\n",
    "    ## Returns:\n",
    "        **entropy_value** : *float, array_like*\\n\n",
    "        The calculated entropy.\n",
    "    \"\"\"\n",
    "    psd_norm = psd / np.sum(psd)        # Normalize the PSD\n",
    "    entropy_value = entropy(psd_norm)   # Compute entropy\n",
    "\n",
    "    return entropy_value\n",
    "\n",
    "\n",
    "def extract_features_freq_domain(signal, fs):\n",
    "    \"\"\"\n",
    "    This function extracts features in the frequency domain from a raw EEG signal\n",
    "\n",
    "    ## Parameters:\n",
    "        **signal**: *array_like*\\n\n",
    "        The EEG signal containing one or more electrodes.\n",
    "\n",
    "        **fs**: *int*\\n\n",
    "        The sampling frequency which the signal was acquired.\n",
    "\n",
    "    ## Returns:\n",
    "        This function returns a *ndarray* which contain, respectively, the following features:\\n\n",
    "    \"\"\"\n",
    "    electrode_features = []\n",
    "\n",
    "    # loop over each electrode\n",
    "    for electrode in signal:\n",
    "\n",
    "        # Compute frequency domain features\n",
    "        freqs, psd = compute_psd(electrode, fs)\n",
    "\n",
    "        delta_power = bandpower(freqs, psd, (0.5, 4))\n",
    "        theta_power = bandpower(freqs, psd, (4, 8))\n",
    "        alpha_power = bandpower(freqs, psd, (8, 12))\n",
    "        beta_power = bandpower(freqs, psd, (12, 30))\n",
    "        gama_power = bandpower(freqs, psd, (30, 45))\n",
    "\n",
    "        mean_freq, median_freq = mean_median_freq(freqs, psd)\n",
    "        spec_entropy = spectral_entropy(psd)\n",
    "\n",
    "        features = np.array([delta_power, theta_power, alpha_power, beta_power, gama_power, mean_freq, median_freq, spec_entropy])\n",
    "        electrode_features = np.concatenate((electrode_features, features))\n",
    "\n",
    "    return electrode_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 1250)\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "print(data_array[0].shape)\n",
    "\n",
    "all_features = extract_features_freq_domain(data_array[0], fs=250)\n",
    "\n",
    "print(len(all_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 1250)\n",
      "171\n"
     ]
    }
   ],
   "source": [
    "print(data_array[0].shape)\n",
    "\n",
    "all_features = extract_features_time_domain(data_array[0])\n",
    "\n",
    "print(len(all_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = []\n",
    "for data in data_array:\n",
    "    all_features.append(extract_features_time_domain(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7201, 171)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features_array = np.array(all_features)\n",
    "all_features_array.shape          # (Nb of epochs, all features extracted from each electrode: 9x19) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**: Logistic regression is a supervised learning algorithm used for binary or multiclass classification. This code creates a machine learning pipeline to train a logistic regression classifier with hyperparameter tuning using grouped cross-validation.\n",
    "\n",
    "*GroupKFold Cross-Validation*:\n",
    "- GroupKFold ensures that samples from the same group (e.g., signals from the same subject) are not mixed between training and test sets. This prevents data leakage and ensures that the model generalizes better to unseen data, improving its predictive performance.\n",
    "- This approach helps when the data has inherent group structure (such as multiple signals from the same individual or session).\n",
    "\n",
    "*GridSearchCV with Hyperparameter Tuning*:\n",
    "- GridSearchCV systematically evaluates different values of the hyperparameters (in this case, the regularization strength **C** in logistic regression) and identifies the best configuration for the model.\n",
    "- The C parameter controls the tradeoff between fitting the model too closely to the training data (overfitting) and underfitting the data.\n",
    "\n",
    "*Standardization*:\n",
    "- The pipeline includes a StandardScaler, which standardizes the features (i.e., transforms them to have zero mean and unit variance). This is particularly helpful for models like logistic regression, which can be sensitive to the scale of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=GroupKFold(n_splits=5),\n",
       "             estimator=Pipeline(steps=[(&#x27;scalar&#x27;, StandardScaler()),\n",
       "                                       (&#x27;clf&#x27;, LogisticRegression())]),\n",
       "             n_jobs=12,\n",
       "             param_grid={&#x27;clf__C&#x27;: array([0.05      , 0.08275862, 0.11551724, 0.14827586, 0.18103448,\n",
       "       0.2137931 , 0.24655172, 0.27931034, 0.31206897, 0.34482759,\n",
       "       0.37758621, 0.41034483, 0.44310345, 0.47586207, 0.50862069,\n",
       "       0.54137931, 0.57413793, 0.60689655, 0.63965517, 0.67241379,\n",
       "       0.70517241, 0.73793103, 0.77068966, 0.80344828, 0.8362069 ,\n",
       "       0.86896552, 0.90172414, 0.93448276, 0.96724138, 1.        ])})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=GroupKFold(n_splits=5),\n",
       "             estimator=Pipeline(steps=[(&#x27;scalar&#x27;, StandardScaler()),\n",
       "                                       (&#x27;clf&#x27;, LogisticRegression())]),\n",
       "             n_jobs=12,\n",
       "             param_grid={&#x27;clf__C&#x27;: array([0.05      , 0.08275862, 0.11551724, 0.14827586, 0.18103448,\n",
       "       0.2137931 , 0.24655172, 0.27931034, 0.31206897, 0.34482759,\n",
       "       0.37758621, 0.41034483, 0.44310345, 0.47586207, 0.50862069,\n",
       "       0.54137931, 0.57413793, 0.60689655, 0.63965517, 0.67241379,\n",
       "       0.70517241, 0.73793103, 0.77068966, 0.80344828, 0.8362069 ,\n",
       "       0.86896552, 0.90172414, 0.93448276, 0.96724138, 1.        ])})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scalar&#x27;, StandardScaler()), (&#x27;clf&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=GroupKFold(n_splits=5),\n",
       "             estimator=Pipeline(steps=[('scalar', StandardScaler()),\n",
       "                                       ('clf', LogisticRegression())]),\n",
       "             n_jobs=12,\n",
       "             param_grid={'clf__C': array([0.05      , 0.08275862, 0.11551724, 0.14827586, 0.18103448,\n",
       "       0.2137931 , 0.24655172, 0.27931034, 0.31206897, 0.34482759,\n",
       "       0.37758621, 0.41034483, 0.44310345, 0.47586207, 0.50862069,\n",
       "       0.54137931, 0.57413793, 0.60689655, 0.63965517, 0.67241379,\n",
       "       0.70517241, 0.73793103, 0.77068966, 0.80344828, 0.8362069 ,\n",
       "       0.86896552, 0.90172414, 0.93448276, 0.96724138, 1.        ])})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model itself\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Ensures that the samples within the same group remain in the same fold during training and testing\n",
    "gkf = GroupKFold(5)\n",
    "\n",
    "# Standardizes features and puts the classifier after the scaler so the data is standardized before training.\n",
    "pipeline = Pipeline([('scalar', StandardScaler()), ('clf', clf)])\n",
    "\n",
    "# Specifies the values to test for the logistic regression C parameter. Each value in this list corresponds to a different regularization strength.\n",
    "param_grid = {'clf__C':np.linspace(0.05, 1, 30)}\n",
    "\n",
    "# GridSearchCV performs a grid search over the param_grid to find the best C parameter for the logistic regression model, \n",
    "# using cross-validation with GroupKFold. The n_jobs parameter allows parallel processing, speeding up the computation by using X CPU cores\n",
    "gscv = GridSearchCV(pipeline, param_grid, cv=gkf, n_jobs=12)\n",
    "\n",
    "# trains the GridSearchCV object using the feature array (all_features_array), labels (label_array), and groups (group_array). \n",
    "# After running this, gscv will contain the best logistic regression model based on the cross-validated performance.\n",
    "gscv.fit(all_features_array, label_array, groups=group_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score Prediction: 70.52 %\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to dict.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Score Prediction: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(gscv\u001b[38;5;241m.\u001b[39mbest_score_\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBest parameter (C): \u001b[39;49m\u001b[38;5;132;43;01m{:.2f}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgscv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params_\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to dict.__format__"
     ]
    }
   ],
   "source": [
    "print(\"Best Score Prediction: {:.2f} %\".format(gscv.best_score_*100))\n",
    "\n",
    "print(\"Best parameter (C): {:.2f}\".format(gscv.best_params_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
